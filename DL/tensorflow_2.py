import tensorflow as tf
import numpy as np
import os

# åˆ†ç±»é—®é¢˜
# load_data()å‡½æ•°è¿”å›ä¸¤ä¸ªå…ƒç»„(tuple)å¯¹è±¡ï¼Œç¬¬ä¸€ä¸ªæ˜¯è®­ç»ƒé›†ï¼Œç¬¬äºŒä¸ªæ˜¯æµ‹è¯•é›†
(x, y), (x_val, y_val) = tf.keras.datasets.mnist.load_data()  # åŠ è½½ MNIST æ•°æ®é›†
# è½¬æ¢ä¸ºæµ®ç‚¹å¼ é‡ï¼Œå¹¶ç¼©æ”¾åˆ°-1~1 :å…ˆå½’ä¸€åŒ–ï¼Œå†å˜ä¸º-1~1
x = 2* tf.convert_to_tensor(x, dtype=tf.float32)/255. -1
y = tf.convert_to_tensor(y, dtype=tf.int32)  # è½¬æ¢ä¸ºæ•´å½¢å¼ é‡
# y = tf.constant([0,1,2,3])   æ•°å­—ç¼–ç çš„ 4 ä¸ªæ ·æœ¬æ ‡ç­¾
y = tf.one_hot(y, depth=10)  # one-hot ç¼–ç 
print(x.shape, y.shape)
train_dataset = tf.data.Dataset.from_tensor_slices((x, y))  # æ„å»ºæ•°æ®é›†å¯¹è±¡
train_dataset = train_dataset.batch(512) # æ‰¹é‡è®­ç»ƒ

"""
å›¾ç‰‡çš„çœŸå®æ ‡ç­¾ğ‘¦ç»è¿‡ One-hot ç¼–ç åå˜æˆé•¿åº¦ä¸º 10 çš„ é 0 å³ 1 çš„ç¨€ç–å‘é‡ğ’š âˆˆ {0,1}
10ã€‚é¢„æµ‹æ¨¡å‹é‡‡ç”¨å¤šè¾“å…¥ã€å¤šè¾“å‡ºçš„çº¿æ€§æ¨¡å‹ = ğ‘¾ğ“ğ’™ + ğ’ƒï¼Œ
å…¶ä¸­æ¨¡å‹çš„è¾“å‡ºè®°ä¸ºè¾“å…¥çš„é¢„æµ‹å€¼ ï¼Œæˆ‘ä»¬å¸Œæœ› è¶Šæ¥è¿‘çœŸå®æ ‡ç­¾ğ’šè¶Šå¥½ã€‚ä¸€èˆ¬æŠŠè¾“å…¥ç»è¿‡
ä¸€æ¬¡(çº¿æ€§)å˜æ¢å«ä½œä¸€å±‚ç½‘ç»œã€‚
"""

# åˆ›å»ºä¸€å±‚ç½‘ç»œï¼Œè®¾ç½®è¾“å‡ºèŠ‚ç‚¹æ•°ä¸º 256ï¼Œæ¿€æ´»å‡½æ•°ç±»å‹ä¸º ReLU
tf.keras.layers.Dense(256, activation='relu')
# åˆ©ç”¨ Sequential å®¹å™¨å°è£… 3 ä¸ªç½‘ç»œå±‚ï¼Œå‰ç½‘ç»œå±‚çš„è¾“å‡ºé»˜è®¤ä½œä¸ºä¸‹ä¸€å±‚çš„è¾“å…¥
model = tf.keras.Sequential([     # 3 ä¸ªéçº¿æ€§å±‚çš„åµŒå¥—æ¨¡å‹
                tf.keras.layers.Dense(256, activation='relu'),  # éšè—å±‚ 1
                tf.keras.layers.Dense(128, activation='relu'),  # éšè—å±‚ 2
                tf.keras.layers.Dense(10)])  # è¾“å‡ºå±‚ï¼Œè¾“å‡ºèŠ‚ç‚¹æ•°ä¸º 10
# ç¬¬ 1 å±‚çš„è¾“å‡ºèŠ‚ç‚¹æ•°è®¾è®¡ä¸º 256ï¼Œç¬¬ 2 å±‚è®¾è®¡ä¸º 128ï¼Œè¾“å‡ºå±‚èŠ‚ç‚¹æ•°è®¾è®¡ä¸º 10ã€‚
# ç›´æ¥è°ƒç”¨è¿™ä¸ªæ¨¡å‹å¯¹è±¡ model(x)å°±å¯ä»¥è¿”å›æ¨¡å‹æœ€åä¸€å±‚çš„è¾“å‡ºğ‘œ

with tf.GradientTape() as tape: # æ„å»ºæ¢¯åº¦è®°å½•ç¯å¢ƒ
    # æ‰“å¹³æ“ä½œï¼Œ[b, 28, 28] => [b, 784]
    x = tf.reshape(x, (-1, 28*28))
    # Step1. å¾—åˆ°æ¨¡å‹è¾“å‡º output [b, 784] => [b, 10]
    out = model(x)
    # [b] => [b, 10]
    y_onehot = tf.one_hot(y, depth=10)  # è®¡ç®—å·®çš„å¹³æ–¹å’Œï¼Œ[b, 10]
    loss = tf.square(out-y_onehot)
    # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å¹³å‡è¯¯å·®ï¼Œ[b]
    loss = tf.reduce_sum(loss) / x.shape[0]
    # å†åˆ©ç”¨tfæä¾›çš„è‡ªåŠ¨æ±‚å¯¼å‡½æ•° tape.gradient(loss, model.trainable_variables)
    # æ±‚å‡ºæ¨¡å‹ä¸­æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦ä¿¡æ¯
    # Step3. è®¡ç®—å‚æ•°çš„æ¢¯åº¦ w1, w2, w3, b1, b2, b3
    grads = tape.gradient(loss, model.trainable_variables)

    # è®¡ç®—è·å¾—çš„æ¢¯åº¦ç»“æœä½¿ç”¨ grads åˆ—è¡¨å˜é‡ä¿å­˜ã€‚å†ä½¿ç”¨ optimizers å¯¹è±¡
    # è‡ªåŠ¨æŒ‰ç…§æ¢¯åº¦æ›´æ–°æ³•åˆ™å»æ›´æ–°æ¨¡å‹çš„å‚æ•°ğœƒã€‚
    # è‡ªåŠ¨è®¡ç®—æ¢¯åº¦
    grads = tape.gradient(loss, model.trainable_variables)
    # w' = w - lr * gradï¼Œæ›´æ–°ç½‘ç»œå‚æ•°
    tf.keras.optimizers.apply_gradients(zip(grads, model.trainable_variables))



# tensorflow深度学习第四章 4.6开始
# 2020-2-16

import tensorflow as tf
import numpy as np

# 索引和切片
# 通过索引与切片操作可以提取张量的部分数据，它们的使用频率非常高。
"""
在 TensorFlow 中，支持基本的[𝑖][𝑗]⋯标准索引方式，也支持通过逗号分隔索引号的索
引方式。考虑输入𝑿为 4 张32 × 32大小的彩色图片(为了方便演示，大部分张量都使用随机
分布模拟产生，后文同)，shape 为[4,32,32,3]，首先创建张量：
x = tf.random.normal([4,32,32,3]) # 创建 4D 张量
接下来我们使用索引方式读取张量的部分数据。

取第 1 张图片的数据，实现如下：
x[0] # 程序中的第一的索引号应为 0，容易混淆，不过不影响理解
取第 1 张图片的第 2 行，实现如下：
x[0][1]
取第 1 张图片，第 2 行，第 3 列的数据，实现如下：
x[0][1][2]
取第 3 张图片，第 2 行，第 1 列的像素，B 通道(第 2 个通道)颜色强度值
x[2][1][0][1]
当张量的维度数较高时，使用[𝑖][𝑗]. . .[𝑘]的方式书写不方便，可以采用[𝑖,𝑗, … , 𝑘]的方
式索引，它们是等价的。
取第 2 张图片，第 10 行，第 3 列的数据     x[1,9,2]

通过start: end: step切片方式可以方便地提取一段数据，其中 start 为开始读取位置的索
引，end 为结束读取位置的索引(不包含 end 位)，step 为采样步长。
以 shape 为[4,32,32,3]的图片张量为例，我们解释如果切片获得不同位置的数据。例如
读取第 2,3 张图片，实现如下：
x[1:3]
start: end: step切片方式有很多简写方式，其中 start、end、step 3 个参数可以根据需要
选择性地省略，全部省略时即为::，表示从最开始读取到最末尾，步长为 1，即不跳过任何
元素。如 x[0,::]表示读取第 1 张图片的所有行，其中::表示在行维度上读取所有行，
它等价 于 x[0]的写法：   
x[0,::] # 读取第一张图片
为了更加简洁，::可以简写为单个冒号:，例如  x[:,0:28:2,0:28:2,:]  
表示读取所有图片、隔行采样、隔列采样，、读取所有通道数据，相当于在图片的高宽上各
缩放至原来的 50%。

我们来总结一下start: end: step切片的简写方式，其中从第一个元素读取时 start 可以省
略，即 start=0 是可以省略，取到最后一个元素时 end 可以省略，步长为 1 时 step 可以省
略
start:end:step 从 start 开始读取到 end(不包含 end)，步长为 step
start:end 从 start 开始读取到 end(不包含 end)，步长为 1 start: 从 start 开始读取完后续所有元素，步长为 1 start::step 从 start 开始读取完后续所有元素，步长为 step
:end:step 从 0 开始读取到 end(不包含 end)，步长为 step
:end 从 0 开始读取到 end(不包含 end)，步长为 1 ::step 步长为 step 采样
:: 读取所有元素
: 读取所有元素

特别地，step 可以为负数，考虑最特殊的一种例子，当step = −1时，start: end: −1表
示从 start 开始，逆序读取至 end 结束(不包含 end)，索引号𝑒𝑛𝑑 ≤ 𝑠𝑡𝑎𝑟𝑡。考虑一个 0~9 的
简单序列向量，逆序取到第 1 号元素，不包含第 1 号
x = tf.range(9) # 创建 0~9 向量
x[8:0:-1] # 从 8 取到 0，逆序，不包含 0
逆序取全部元素，实现如下：
x[::-1] # 逆序全部元素
逆序间隔采样，实现如下：
x[::-2] # 逆序间隔采样
读取每张图片的所有通道，其中行按着逆序隔行采样，列按着逆序隔行采样，实现如下：
x = tf.random.normal([4,32,32,3])
x[0,::-2,::-2] # 行、列逆序间隔采样
当张量的维度数量较多时，不需要采样的维度一般用单冒号:表示采样所有元素，此时
有可能出现大量的:出现。继续考虑[4,32,32,3]的图片张量，当需要读取 G 通道上的数据
时，前面所有维度全部提取，此时需要写为：
x[:,:,:,1] # 取 G 通道数据


"""

"""
为了避免出现像 [: , : , : ,1]这样过多冒号的情况，可以使用⋯符号表示取多个维度上所
有的数据，其中维度的数量需根据规则自动推断：当切片方式出现⋯符号时，⋯符号左边
的维度将自动对齐到最左边，⋯符号右边的维度将自动对齐到最右边，此时系统再自动推
断⋯符号代表的维度数量，它的切片方式总结

a,⋯,b a 维度对齐到最左边，b 维度对齐到最右边，中间的维度全部读取，
其他维度按 a/b 的方式读取
a,⋯ a 维度对齐到最左边，a 维度后的所有维度全部读取，a 维度按 a 方式
读取。这种情况等同于 a 索引/切片方式
⋯,b b 维度对齐到最右边，b 之前的所有维度全部读取，b 维度按 b 方式
读取
⋯ 读取张量所有数据

读取第 1~2 张图片的 G/B 通道数据   x[0:2,...,1:] # 高宽维度全部采集
读取最后 2 张图片  x[2:,...] # 高、宽、通道维度全部采集，等价于 x[2:]
读取 R/G 通道数据  x[...,:2] # 所有样本，所有高、宽的前 2 个通道

"""

"""
在神经网络运算过程中，维度变换是最核心的张量操作，通过维度变换可以将数据任
意地切换形式，满足不同场合的运算需求。
那么为什么需要维度变换呢？考虑线性层的批量形式：
𝒀 = 𝑿@𝑾 + b

其中，假设𝑿包含了 2 个样本，每个样本的特征长度为 4，𝑿的 shape 为[2,4]。线性层的输
出为 3 个节点，即𝑾的 shape 定义为[4,3]，偏置𝒃的 shape 定义为[3]。那么X@W的运算结 果张量 shape 为[2,3]，需要叠加上 shape 为[3]的偏置𝒃。不同 shape 的 2 个张量怎么直接相
加呢？
回顾设计偏置的初衷，我们给每个层的每个输出节点添加一个偏置，这个偏置数据是
对所有的样本都是共享的，换言之，每个样本都应该累加上同样的偏置向量𝒃，
因此，对于 2 个样本的输入𝑿，我们需要将 shape 为[3]的偏置
按样本数量复制 1 份  𝑩′ = [𝑏1 𝑏2 𝑏 𝑏1 𝑏2 𝑏 ]
相加，此时X′与𝐵′shape 相同，满足矩阵相加的数学条件

通过这种方式，既满足了数学上矩阵相加需要 shape 一致的条件，又达到了给每个输入样
本的输出节点共享偏置向量的逻辑。为了实现这种运算方式，我们将偏置向量𝒃插入一个
新的维度，并把它定义为 Batch 维度，然后在 Batch 维度将数据复制 1 份，得到变换后的
𝐁′，新的 shape 为[2,3]。这一系列的操作就是维度变换操作。
算法的每个模块对于数据张量的格式有不同的逻辑要求，当现有的数据格式不满足算
法要求时，需要通过维度变换将数据调整为正确的格式。这就是维度变换的功能。
基本的维度变换操作函数包含了改变视图 reshape、插入新维度 expand_dims，删除维
度 squeeze、交换维度 transpose、复制数据 tile 等函数。
"""

# 4.7.1
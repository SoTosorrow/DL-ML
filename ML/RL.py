# 2020-1-20

# agent,state,action,reward,policy\

# 马尔可夫决策要求：
# 1，能够检测到理想的状态。
# 2，可以多次尝试。
# 3，系统的下个状态只与当前状态信息有关，而与更早之前的状态无关，在决策过程中还和当前采取的动作有关

# 马尔可夫决策过程五要素
# 1，S 表示状态集 states
# 2，A 表示一组动作 actions
# 3，P 表示状态转移概率 Psa 表示在当前s 属于 S时，经过一个属于A的动作a后，
#       会转移到其他的状态的概率分布情况在状态s下执行动作a，转移到s2的概率可以表示为p（s2|s，a）
# 4，R 奖励函数 reward function 表示 agent 采取某个动作后的即时奖励
# 5，y 折扣系数。意味着当下的reward比未来反馈的reward更重要

# 状态价值函数 v(s) = E[Ut|St = s]
# t时刻的状态s能获得的未来回报的期望
# 价值函数用来衡量某一状态或状态动作对的优劣价，累计奖励的期望
# 最优价值函数，所以策略下的最优累计奖励期望 v*(s)=max v(s)
# 策略：已知状态下可能产生动作的概率分布

# Ballman方程：当前状态的价值和下一步的价值及当前的奖励有关，价值函数分解为当前的奖励和下一步的价值两部分
